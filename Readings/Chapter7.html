<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Susan Eileen Fox">
<meta name="dcterms.date" content="2025-10-29">

<title>Chapter 7, Detecting and Tracking Objects (without ML) – Vision Readings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-226bd0f977fa82dfae4534cac220d79a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9011e249e8d359b0658fa71d60c1fa6f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Vision Readings</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 7, Detecting and Tracking Objects (without ML)</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Susan Eileen Fox </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 29, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#finding-contours" id="toc-finding-contours" class="nav-link active" data-scroll-target="#finding-contours"><span class="header-section-number">1</span> Finding contours</a>
  <ul class="collapse">
  <li><a href="#inputs-to-findcontours" id="toc-inputs-to-findcontours" class="nav-link" data-scroll-target="#inputs-to-findcontours"><span class="header-section-number">1.1</span> Inputs to <code>findContours</code></a></li>
  <li><a href="#outputs-from-findcontours" id="toc-outputs-from-findcontours" class="nav-link" data-scroll-target="#outputs-from-findcontours"><span class="header-section-number">1.2</span> Outputs from <code>findContours</code></a></li>
  <li><a href="#an-example-of-findcontours" id="toc-an-example-of-findcontours" class="nav-link" data-scroll-target="#an-example-of-findcontours"><span class="header-section-number">1.3</span> An example of <code>findContours</code></a></li>
  <li><a href="#drawing-contours" id="toc-drawing-contours" class="nav-link" data-scroll-target="#drawing-contours"><span class="header-section-number">1.4</span> Drawing contours</a></li>
  <li><a href="#selecting-contours-by-area" id="toc-selecting-contours-by-area" class="nav-link" data-scroll-target="#selecting-contours-by-area"><span class="header-section-number">1.5</span> Selecting contours by area</a></li>
  <li><a href="#functions-to-manipulate-contours" id="toc-functions-to-manipulate-contours" class="nav-link" data-scroll-target="#functions-to-manipulate-contours"><span class="header-section-number">1.6</span> Functions to manipulate contours</a>
  <ul class="collapse">
  <li><a href="#bounding-rectangle" id="toc-bounding-rectangle" class="nav-link" data-scroll-target="#bounding-rectangle"><span class="header-section-number">1.6.1</span> Bounding rectangle</a></li>
  <li><a href="#minimum-enclosing-circle" id="toc-minimum-enclosing-circle" class="nav-link" data-scroll-target="#minimum-enclosing-circle"><span class="header-section-number">1.6.2</span> Minimum enclosing circle</a></li>
  <li><a href="#best-fit-ellipse" id="toc-best-fit-ellipse" class="nav-link" data-scroll-target="#best-fit-ellipse"><span class="header-section-number">1.6.3</span> Best-fit ellipse</a></li>
  <li><a href="#convex-hull" id="toc-convex-hull" class="nav-link" data-scroll-target="#convex-hull"><span class="header-section-number">1.6.4</span> Convex hull</a></li>
  <li><a href="#minimum-enclosing-triangle" id="toc-minimum-enclosing-triangle" class="nav-link" data-scroll-target="#minimum-enclosing-triangle"><span class="header-section-number">1.6.5</span> Minimum enclosing triangle</a></li>
  <li><a href="#results-of-drawing-shapes-for-contours" id="toc-results-of-drawing-shapes-for-contours" class="nav-link" data-scroll-target="#results-of-drawing-shapes-for-contours"><span class="header-section-number">1.6.6</span> Results of drawing shapes for contours</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#motion-detection-and-background-subtraction" id="toc-motion-detection-and-background-subtraction" class="nav-link" data-scroll-target="#motion-detection-and-background-subtraction"><span class="header-section-number">2</span> Motion detection and background subtraction</a></li>
  <li><a href="#background-subtraction" id="toc-background-subtraction" class="nav-link" data-scroll-target="#background-subtraction"><span class="header-section-number">3</span> Background subtraction</a>
  <ul class="collapse">
  <li><a href="#a-simple-attempt-at-background-subtraction" id="toc-a-simple-attempt-at-background-subtraction" class="nav-link" data-scroll-target="#a-simple-attempt-at-background-subtraction"><span class="header-section-number">3.1</span> A simple attempt at background subtraction</a></li>
  <li><a href="#the-mog-mixture-of-gaussians-approach-to-background-subtraction" id="toc-the-mog-mixture-of-gaussians-approach-to-background-subtraction" class="nav-link" data-scroll-target="#the-mog-mixture-of-gaussians-approach-to-background-subtraction"><span class="header-section-number">3.2</span> The MOG (Mixture of Gaussians) approach to background subtraction</a></li>
  <li><a href="#the-knn-k-nearest-neighbor-approach-to-background-subtraction" id="toc-the-knn-k-nearest-neighbor-approach-to-background-subtraction" class="nav-link" data-scroll-target="#the-knn-k-nearest-neighbor-approach-to-background-subtraction"><span class="header-section-number">3.3</span> The KNN (K-nearest neighbor) approach to background subtraction</a></li>
  </ul></li>
  <li><a href="#color-tracking-with-camshift" id="toc-color-tracking-with-camshift" class="nav-link" data-scroll-target="#color-tracking-with-camshift"><span class="header-section-number">4</span> Color-tracking with CAMShift</a>
  <ul class="collapse">
  <li><a href="#histograms-and-backprojection" id="toc-histograms-and-backprojection" class="nav-link" data-scroll-target="#histograms-and-backprojection"><span class="header-section-number">4.1</span> Histograms and Backprojection</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This chapter will look at several approaches that allow us to identify objects in images, and to track motion and color. The first section will look at tools for finding and manipulating <strong>contours</strong>, arrays of (x, y) coordinage points that form a closed polygon on the image. The next section will examine basic motion detection, and <em>background subtraction</em> algorithms, and the third section will look at the Camshift color tracking algorithm.</p>
<p>(Later sections, if added, will examine line and circle detection, as well as corner detection)</p>
<section id="finding-contours" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Finding contours</h1>
<p>All the image processing we’ve done in previous chapters has ended up transforming one image into another. The goal of computer vision is <strong>to get information out of images</strong>, but we haven’t really done that yet. Finding contours will be the first approach we will look at that does give us information. A contour describes a section of the image defined by a list of points. The points form a closed shape, a polygon. The polygon lies along a bright edge in the image.</p>
<p>The original contour algorithm was described in a paper by Satoshi Suzuki and Keiichi Abe in 1985 (<em>Toplogical Structure Analysis of Digitized Binary Images by Border Following</em>). The algorithm operates on a <em>binary image,</em> where all pixels are either white or black. If you give it a color image, its results are typically very noisy.</p>
<p>The <code>findContours</code> algorithm scans the image for a pixel that sits on an edge (where bright and dark meet) (as shown in <a href="#fig-contours" class="quarto-xref">Figure&nbsp;1</a> below). It then determines neighbor pixels that are also edges, and follows the neighbors until it returns to the starting point, having traced a full closed shape on the image that lies along an edge. It repeats this task for the rest of the image, and returns a list of all the contours it found (as shown in <a href="#fig-contours" class="quarto-xref">Figure&nbsp;1</a>).</p>
<div id="fig-contours" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-contours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/contourAlg1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="At the start, finding an edge pixel and searching for neighbors"><img src="Ch7-Images/contourAlg1.png" class="img-fluid figure-img"></a></p>
<figcaption>At the start, finding an edge pixel and searching for neighbors</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/contourAlg2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="At the end, having found five contours"><img src="Ch7-Images/contourAlg2.png" class="img-fluid figure-img"></a></p>
<figcaption>At the end, having found five contours</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-contours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Early and late in the contour-finding process
</figcaption>
</figure>
</div>
<section id="inputs-to-findcontours" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="inputs-to-findcontours"><span class="header-section-number">1.1</span> Inputs to <code>findContours</code></h2>
<p>OpenCV implements this algorithm with the <code>findContours</code> function. This function takes in three inputs, and returns two values. The three inputs are:</p>
<ul>
<li>the binary image to be processed</li>
<li>a code specifying the <strong>mode</strong> of the search, and</li>
<li>a code specifying how much to approximate or optimize the contour that is found</li>
</ul>
<p>There are five different modes for the <code>findContours</code> search, described in the table below. <strong>For our purposes, we will really only need <code>cv2.RETR_LIST</code>.</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Mode</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>cv2.RETR_EXTERNAL</code></td>
<td style="text-align: left;">Retrieves only outermost contours, doesn’t keep any contours inside of other contours</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>cv2.RETR_LIST</code></td>
<td style="text-align: left;">Retrieves all contours without any hierarchical information about which contours are inside which others</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cv2.RETR_TREE</code></td>
<td style="text-align: left;">Retrieves all contours and constructs a tree representation of which contours are inside which others</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>cv2.RETR_CCOMP</code></td>
<td style="text-align: left;">Retrieves outer contours and connects them to holes in side of them but treats contours inside of holes as independent contours (CCOMP stands for “connected components” which is a mathematical term for these structures)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cv2.RETR_FLOODFILL</code></td>
<td style="text-align: left;">Uses a “flood fill” algorithm to build up regions of similar brightness or intensity, then makes contours of their borders (in essence, a completely different algorithm!)</td>
</tr>
</tbody>
</table>
<p>The third input specified the “contour approximation mode”. This determines how much the contour is simplified. The two main modes are <code>cv2.CHAIN_APPROX_NONE</code> and <code>cv2.CHAIN_APPROX_SIMPLE</code>, though there are more complex modes that are rarely used. The <code>NONE</code> mode does no optimization or approximation of the contours. Every border pixel is included in the contour array. The <code>SIMPLE</code> mode looks for horizontal, vertical, or pure diagonal segments of the contour, and stores only the end points, rather than every pixel.</p>
</section>
<section id="outputs-from-findcontours" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="outputs-from-findcontours"><span class="header-section-number">1.2</span> Outputs from <code>findContours</code></h2>
<p>As noted above, <code>findContours</code> returns two values. The first is a list of contour arrays, all the contours retrieved by the algorithm. The second return value is a <em>hierarchy</em> object that represents how contours are nested with each other. We won’t typically use this, but it does play a role in tasks like recognizing QR codes!</p>
</section>
<section id="an-example-of-findcontours" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="an-example-of-findcontours"><span class="header-section-number">1.3</span> An example of <code>findContours</code></h2>
<p>The code block below contains a short example of how to prepare an image for <code>findContours</code> and what it returns. In the code, we read in a picture of a peony flower, convert it to grayscale, and then perform a simple threshold to split all pixels brighter than 80 to 255, and all darker to 0.</p>
<p>We then pass the threshold image to <code>findContours</code>, with typical mode inputs (<code>cv2.RETR_LIST</code> and <code>cv2.CHAIN_APPROX_SIMPLE</code>). We assign the two return values to the two variables <code>contours</code> and <code>hierarchy</code>. We won’t use <code>hierarchy</code> further.</p>
<p>The <code>contours</code> variable holds a list of contour arrays. Just to get a sense of them, the code prints the number of contours found, <strong>a surprising 68</strong>, and also print the values of the first two contours in the list. Each contour ach contour is an array of (x, y) coordinates, representing the pixels that form the contour.</p>
<div id="f0850dbc" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>flowerPic <span class="op">=</span> cv2.imread(<span class="st">"Ch7-Images/peony.jpg"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>grayFlower <span class="op">=</span> cv2.cvtColor(flowerPic, cv2.COLOR_BGR2GRAY)</span>
<span id="cb1-3"><a href="#cb1-3"></a>thr, threshPic <span class="op">=</span> cv2.threshold(grayFlower, <span class="dv">80</span>, <span class="dv">255</span>, cv2.THRESH_BINARY)</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a>contours, hierarchy <span class="op">=</span> cv2.findContours(threshPic, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="bu">print</span>(<span class="bu">len</span>(contours))</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="bu">print</span>(<span class="st">"First contour:"</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="bu">print</span>(contours[<span class="dv">0</span>])</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="bu">print</span>(<span class="st">"Second contour:"</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="bu">print</span>(contours[<span class="dv">1</span>]) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>68
First contour:
[[[350 427]]

 [[351 426]]

 [[352 427]]

 [[351 428]]]
Second contour:
[[[350 424]]

 [[351 423]]

 [[352 424]]

 [[351 425]]]</code></pre>
</div>
</div>
<p>Look closely at the contours that were printed. Each of them is only four pixels long. If you work out the locations of the four pixels, you will see that they form a diamond above, below, to the left, and to the right of <strong>a single pixel.</strong> These are the smallest possible contours, and usually not of interest to us. Many of the 68 contours are marking tiny patches of noise in the image (a demonstration of why we might want to blur or morph an image to get rid of these!).</p>
</section>
<section id="drawing-contours" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="drawing-contours"><span class="header-section-number">1.4</span> Drawing contours</h2>
<p>OpenCV also provides a helper function, <code>drawContours</code> for drawing contours on an image. The code snippet below makes a copy of the original flower picture, and calls <code>drawContours</code> on it to draw all the contours on it. <a href="#fig-allContours1" class="quarto-xref">Figure&nbsp;2</a> shows the result of this code snippet.</p>
<div id="8e657e9f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>fl <span class="op">=</span> flowerPic.copy()</span>
<span id="cb3-2"><a href="#cb3-2"></a>cv2.drawContours(fl, contours, <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>cv2.imshow(<span class="st">"All Contours"</span>, fl)</span>
<span id="cb3-4"><a href="#cb3-4"></a>cv2.waitKey()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We pass at least five inputs to <code>drawContours</code>:</p>
<ul>
<li>the image to draw on</li>
<li>a list of the contours</li>
<li>an index into the list showing which contour to draw, or -1 if we want all contours drawn</li>
<li>A color for the contour polygon to be drawn with</li>
<li>A line thickness, or -1 for a filled in polygon</li>
</ul>
<div id="fig-allContours1" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-allContours1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="Ch7-Images/allContours1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;2: All contours drawn in green on the original peony picture"><img src="Ch7-Images/allContours1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-allContours1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: All contours drawn in green on the original peony picture
</figcaption>
</figure>
</div>
</section>
<section id="selecting-contours-by-area" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="selecting-contours-by-area"><span class="header-section-number">1.5</span> Selecting contours by area</h2>
<p>We often need to reduce our list of contours to those that are of significant size to get rid of the noisy ones. The code block below, an extension of the previous one, introduces one of the many OpenCV functions that operate on contours: <code>cv2.contourArea</code>, which computes how many pixels are enclosed by a contour. We use it to keep only contours that enclose at least 100 pixels.</p>
<div id="e1d52878" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>bigContours <span class="op">=</span> []</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="cf">for</span> contr <span class="kw">in</span> contours:</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="cf">if</span> cv2.contourArea(contr) <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb4-4"><a href="#cb4-4"></a>        bigContours.append(contr)</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="bu">print</span>(<span class="st">"Number of big contours:"</span>, <span class="bu">len</span>(bigContours))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of big contours: 4</code></pre>
</div>
</div>
<p><a href="#fig-allContours2" class="quarto-xref">Figure&nbsp;3</a> shows the result of drawing just the big contours on a copy of the original image.</p>
<div id="5fb624ac" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>fl <span class="op">=</span> flowerPic.copy()</span>
<span id="cb6-2"><a href="#cb6-2"></a>cv2.drawContours(fl, bigContours, <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a>cv2.imshow(<span class="st">"Big Contours"</span>, fl)</span>
<span id="cb6-4"><a href="#cb6-4"></a>cv2.imwrite(<span class="st">"allContours2.png"</span>, fl)</span>
<span id="cb6-5"><a href="#cb6-5"></a>cv2.waitKey()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-allContours2" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-allContours2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="Ch7-Images/allContours2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;3: Only contours with area above 100 drawn in green"><img src="Ch7-Images/allContours2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-allContours2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Only contours with area above 100 drawn in green
</figcaption>
</figure>
</div>
<p>We can also view contours in isolation, by drawing them separately on either the original image or onto a blank image of the same shape. The code snippet below shows how to draw one contour at a time onto a black image. Note that the black image is recreated each time so that the contours show up separately from each other. <a href="#fig-yellowContours" class="quarto-xref">Figure&nbsp;4</a> shows each contour’s drawing, including one of the tiny contours that would included if this code ran on the full contour list.</p>
<div id="3262166e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bigContours)):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    bc <span class="op">=</span> bigContours[i]</span>
<span id="cb7-3"><a href="#cb7-3"></a>    blankIm <span class="op">=</span> np.zeros(flowerPic.shape, flowerPic.dtype)</span>
<span id="cb7-4"><a href="#cb7-4"></a>    cv2.drawContours(blankIm, bigContours, i, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5"></a>    cv2.imshow(<span class="st">"Contour"</span>, blankIm)</span>
<span id="cb7-6"><a href="#cb7-6"></a>    cv2.waitKey()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice how the call to <code>drawContours</code> is different here. Instead of -1 for the index input, we are passing in an index into the list, so it draws only that contour onto the blank image.</p>
<div id="fig-yellowContours" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-yellowContours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/bigContours0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="First large contour"><img src="Ch7-Images/bigContours0.png" class="img-fluid figure-img"></a></p>
<figcaption>First large contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/bigContours1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Second large contour"><img src="Ch7-Images/bigContours1.png" class="img-fluid figure-img"></a></p>
<figcaption>Second large contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/bigContours2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Third large contour"><img src="Ch7-Images/bigContours2.png" class="img-fluid figure-img"></a></p>
<figcaption>Third large contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/bigContours3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Fourth large contour"><img src="Ch7-Images/bigContours3.png" class="img-fluid figure-img"></a></p>
<figcaption>Fourth large contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/contours0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Example tiny contour (first in overall contours list"><img src="Ch7-Images/contours0.png" class="img-fluid figure-img"></a></p>
<figcaption>Example tiny contour (first in overall <code>contours</code> list</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-yellowContours-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The four large contours from our example, drawn separately onto a black image, plus the first contour from the original list, drawn on a black image.
</figcaption>
</figure>
</div>
</section>
<section id="functions-to-manipulate-contours" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="functions-to-manipulate-contours"><span class="header-section-number">1.6</span> Functions to manipulate contours</h2>
<p>OpenCV provides many functions that can manipulate contours; the <a href="https://docs.opencv.org/4.12.0/d3/dc0/group__imgproc__shape.html#ga17ed9f5d79ae97bd4c7cf18403e1689a">Structural Analysis and Shape Descriptors documentation</a> page lists many more than we will list here. With these functions we can compute simple geometric figures such as polygons, ellipses, or circles that are close in size to the contour, we can simplify the contour, and we can compute useful statistics such as the area enclosed by the contour.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Mode</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>cv2.drawContour(...)</code></td>
<td style="text-align: left;">Draws contours onto an input image with givens color and line</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>cv2.contourArea(contr)</code></td>
<td style="text-align: left;">Returns the number of pixels enclosed by the contour</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cv2.boundingRect(contr)</code></td>
<td style="text-align: left;">Returns a tuple describing the rectangle that bounds the contour: (upperleftX, upperleftY, width, height) (can draw with <code>cv2.rectangle</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>cv2.convexHull(contr)</code></td>
<td style="text-align: left;">Returns a new contour that is a convex polygon fitted around the original contour</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cv2.fitEllipse(contr)</code></td>
<td style="text-align: left;">Returns an ellipse (or a rotated rectangle) that minimizes least-squares error for the contour (can draw with <code>cv2.ellipse</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>cv2.minEnclosingCircle(contr)</code></td>
<td style="text-align: left;">Returns the center and radius (float values) of the minimum-radius circle that encloses the contour (can draw with <code>cv2.circle</code>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cv2.minEnclosingTriangle(contr)</code></td>
<td style="text-align: left;">Returns the vertices of the minimum-area triangle that encloses the contour (can draw with calls to <code>cv2.line</code> for each pair of vertices)</td>
</tr>
</tbody>
</table>
<p>Each code snippet below illustrates how to call each of the five <strong>new</strong> functions from the table above, where <code>bc</code> is one of the contours from <code>bigContours</code> above, and <code>fl</code> is a copy of the original peony picture. <a href="#fig-contShapes" class="quarto-xref">Figure&nbsp;5</a> shows the results of these code snippets on two of the contours.</p>
<section id="bounding-rectangle" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="bounding-rectangle"><span class="header-section-number">1.6.1</span> Bounding rectangle</h3>
<p>A <strong>bounding rectangle</strong> is the smallest rectangle that encloses a certain shape, in this case, a contour. Given a contour, the <code>boundingRect</code> function returns a tuple that describes the bounding rectangle, and which can be passed to <code>cv2.rectangle</code> in place of the two tuples we used before.</p>
<div id="f9b278f7" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>cv2.drawContours(fl, [bc], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2"></a>rect <span class="op">=</span> cv2.boundingRect(bc)</span>
<span id="cb8-3"><a href="#cb8-3"></a>cv2.rectangle(fl, rect, (<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">0</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code first draws just the selected contour on the image, in yellow with one pixel’s thickness. It then computes the bounding rectangle, and draws that on the same image in cyan, with two pixels’ thickness. See the first row of <a href="#fig-contShapes" class="quarto-xref">Figure&nbsp;5</a> below for two examples.</p>
<p>Besides drawing the shapes so that we can visualize the results, the bounding rectangle might be used to define an ROI that contains a targeted object, or if we were looking for a rectangular object, we could decide if this was it by comparing the area of the contour itself with the area of the rectangle.</p>
</section>
<section id="minimum-enclosing-circle" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="minimum-enclosing-circle"><span class="header-section-number">1.6.2</span> Minimum enclosing circle</h3>
<p>A <strong>minimum enclosing circle</strong> is the smallest circle that completely encloses a shape. Given a contour, the <code>minEnclosingCircle</code> function returns a tuple for the circle. The outer tuple contains an inner tuple for the center point of the circle, and a radius for the circle. From this, we could compute the area of the circle or track its position. If we want to visualize it, unfortunately we need to convert the center point and radius to integer values instead of floating-point ones. Then we can use <code>cv2.circle</code> to view the circle that was found.</p>
<div id="b54c61ea" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>cv2.drawContours(fl, [bc], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>circle <span class="op">=</span> cv2.minEnclosingCircle(bc)</span>
<span id="cb9-3"><a href="#cb9-3"></a>cent, rad <span class="op">=</span> circle</span>
<span id="cb9-4"><a href="#cb9-4"></a>cx <span class="op">=</span> <span class="bu">int</span>(cent[<span class="dv">0</span>])</span>
<span id="cb9-5"><a href="#cb9-5"></a>cy <span class="op">=</span> <span class="bu">int</span>(cent[<span class="dv">1</span>])</span>
<span id="cb9-6"><a href="#cb9-6"></a>rad <span class="op">=</span> <span class="bu">int</span>(rad)</span>
<span id="cb9-7"><a href="#cb9-7"></a>cv2.circle(fl, (cx, cy), rad, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">255</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code first draws just the selected contour on the image, in yellow with one pixel’s thickness. It then computes the enclosing circle, converts its center and radius to integers, and draws a red circle on the image. See the second row below for two examples.</p>
<p>As for rectangles, we can visualize the circle, but we could also compare its area to the area of the contour to see how round our contour object is, in order to determine whether it is the object we are looking for.</p>
</section>
<section id="best-fit-ellipse" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="best-fit-ellipse"><span class="header-section-number">1.6.3</span> Best-fit ellipse</h3>
<p>A <strong>best-fit ellipse</strong> is an ellipse that best fits the points in the contour. In this case, it computes the least-squares error between the target ellipse and the contour, and minimizes that. In a sense, it is assuming the contour is supposed to describe an ellipse, and producing the best approximation to that ellipse that it can.</p>
<div id="f9a3280f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>cv2.drawContours(fl, [bc], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>ellipse <span class="op">=</span> cv2.fitEllipse(bc)</span>
<span id="cb10-3"><a href="#cb10-3"></a>cv2.ellipse(fl,ellipse, (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code first draws just the selected contour on the image, in yellow with one pixel’s thickness. It then computes the best-fit ellipse, and draws a blue ellipse on the image. Notice that we can pass the ellipse tuple from <code>fitEllipse</code> directly to the <code>ellipse</code> function to draw it, bypassing all the typical inputs needed to draw an ellipse. See the third row below for examples.</p>
</section>
<section id="convex-hull" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="convex-hull"><span class="header-section-number">1.6.4</span> Convex hull</h3>
<p>A <strong>convex hull</strong> is formed using a subset of the points in the contour, but ensuring that they form a convex polygon: a polygon with no indentations in its surface. Imaging stretching a rubber band around the outside of the contour: the points that it touches form the convex hull. Note that the returned value is another contour array: just one that describes a subset of the points from the original.</p>
<div id="153557ad" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>cv2.drawContours(fl, [bc], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a>hullContour <span class="op">=</span> cv2.convexHull(bc)</span>
<span id="cb11-3"><a href="#cb11-3"></a>cv2.drawContours(fl, [hullContour], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code first draws just the selected contour on the image, in yellow with one pixel’s thickness. It then computes the convex hull, and draws a green hull contour on the image. The fourth row in <a href="#fig-contShapes" class="quarto-xref">Figure&nbsp;5</a> below shows examples.</p>
<p>The convex hull is often used to clean up shapes where we have only partially matched the object with our threshold image. It can fill in for missing parts or places where an object is partially hidden by other objects. We can also use it, along with some other functions found on the documentation page, to count and measure the size of the indentations. This can be used to determine, for instance, how many fingers someone is holding up in front of the camera.</p>
</section>
<section id="minimum-enclosing-triangle" class="level3" data-number="1.6.5">
<h3 data-number="1.6.5" class="anchored" data-anchor-id="minimum-enclosing-triangle"><span class="header-section-number">1.6.5</span> Minimum enclosing triangle</h3>
<p>A <strong>minimum enclosing triangle</strong> is the smallest triangle that completely contains the contour’s points. While less commonly used than the others, this one could be very useful in finding triangular signs, cones, or other triangular shapes. The <code>minEnclosingTriangle</code> function returns the area of the triangle it found, and an array containing three (x, y) coordinates. Because these are all floating-point values, and an interesting structure, it takes a bit of work to extract the values from the returned array, and convert them to integers.</p>
<div id="a224da2e" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>cv2.drawContours(fl, [bc], <span class="op">-</span><span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a>area, triVerts <span class="op">=</span> cv2.minEnclosingTriangle(bc)</span>
<span id="cb12-3"><a href="#cb12-3"></a>[x0, y0] <span class="op">=</span> [<span class="bu">int</span>(v) <span class="cf">for</span> v <span class="kw">in</span> triVerts[<span class="dv">0</span>, <span class="dv">0</span>]]</span>
<span id="cb12-4"><a href="#cb12-4"></a>[x1, y1] <span class="op">=</span> [<span class="bu">int</span>(v) <span class="cf">for</span> v <span class="kw">in</span> triVerts[<span class="dv">1</span>, <span class="dv">0</span>]]</span>
<span id="cb12-5"><a href="#cb12-5"></a>[x2, y2] <span class="op">=</span> [<span class="bu">int</span>(v) <span class="cf">for</span> v <span class="kw">in</span> triVerts[<span class="dv">2</span>, <span class="dv">0</span>]]</span>
<span id="cb12-6"><a href="#cb12-6"></a>cv2.line(fl, (x0, y0), (x1, y1), (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">255</span>), <span class="dv">2</span>)</span>
<span id="cb12-7"><a href="#cb12-7"></a>cv2.line(fl, (x1, y1), (x2, y2), (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">255</span>), <span class="dv">2</span>)</span>
<span id="cb12-8"><a href="#cb12-8"></a>cv2.line(fl, (x2, y2), (x0, y0), (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">255</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This code first draws just the selected contour on the image, in yellow with one pixel’s thickness. It then computes the three corners of the triangle, and draws a magenta triangle between them, using the <code>cv2.line</code> function. The last row of <a href="#fig-contShapes" class="quarto-xref">Figure&nbsp;5</a> shows the result on the ant and peony contours.</p>
</section>
<section id="results-of-drawing-shapes-for-contours" class="level3" data-number="1.6.6">
<h3 data-number="1.6.6" class="anchored" data-anchor-id="results-of-drawing-shapes-for-contours"><span class="header-section-number">1.6.6</span> Results of drawing shapes for contours</h3>
<p>The results below were generated using the code snippets above on the ant and peony contours found earlier.</p>
<div id="fig-contShapes" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-contShapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/rect2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Bounding rectangle for ant contour"><img src="Ch7-Images/rect2.png" class="img-fluid figure-img"></a></p>
<figcaption>Bounding rectangle for ant contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/rect3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Bounding rectangle for peony contour"><img src="Ch7-Images/rect3.png" class="img-fluid figure-img"></a></p>
<figcaption>Bounding rectangle for peony contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/circ2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Minimum enclosing circle for ant contour"><img src="Ch7-Images/circ2.png" class="img-fluid figure-img"></a></p>
<figcaption>Minimum enclosing circle for ant contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/circ3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Minimum enclosing circle for peony contour"><img src="Ch7-Images/circ3.png" class="img-fluid figure-img"></a></p>
<figcaption>Minimum enclosing circle for peony contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/ellipse2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Best-fit ellipse for ant contour"><img src="Ch7-Images/ellipse2.png" class="img-fluid figure-img"></a></p>
<figcaption>Best-fit ellipse for ant contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/ellipse3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Best-fit ellipse for peony contour"><img src="Ch7-Images/ellipse3.png" class="img-fluid figure-img"></a></p>
<figcaption>Best-fit ellipse for peony contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/convhull2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Convex hull for ant contour"><img src="Ch7-Images/convhull2.png" class="img-fluid figure-img"></a></p>
<figcaption>Convex hull for ant contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/convhull3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Convex hull for peony contour"><img src="Ch7-Images/convhull3.png" class="img-fluid figure-img"></a></p>
<figcaption>Convex hull for peony contour</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/tri2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Minimum enclosing triangle for ant contour"><img src="Ch7-Images/tri2.png" class="img-fluid figure-img"></a></p>
<figcaption>Minimum enclosing triangle for ant contour</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/tri3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Minimum enclosing triangle for peony contour"><img src="Ch7-Images/tri3.png" class="img-fluid figure-img"></a></p>
<figcaption>Minimum enclosing triangle for peony contour</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-contShapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The four large contours from our example, drawn separately onto a black image, plus the first contour from the original list, drawn on a black image.
</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="motion-detection-and-background-subtraction" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Motion detection and background subtraction</h1>
<p>Detecting motion in a video signal is deceptively simple. Making sense of what is moving is much harder, and tracking a moving object is also not trivial. In the first case, we can detect changes from frame to frame in a video signal, and know that whatever is different represents movement in the image. Getting from there to distinguishing between a person moving through a scene, and the movement of trees in the wind, or rippling water, is where things get harder. Separating some movement from the rest and knowing that a particular object is moving takes even more processing.</p>
<p>But we’ll start with the easy step first. The most simple method for detecting motion is simply to take the difference between two frames of an image. The code fragment below illustrates this. Work through what each line does, and then try it out.</p>
<div id="1bf26285" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> cv2</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>cam <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a>ret, prevFrame <span class="op">=</span> cam.read()</span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb13-7"><a href="#cb13-7"></a>    gotFrame, currFrame <span class="op">=</span> cam.read()</span>
<span id="cb13-8"><a href="#cb13-8"></a>    <span class="cf">if</span> <span class="kw">not</span> gotFrame:</span>
<span id="cb13-9"><a href="#cb13-9"></a>        <span class="cf">break</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>    diff1 <span class="op">=</span> cv2.absdiff(prevFrame, currFrame)</span>
<span id="cb13-11"><a href="#cb13-11"></a>    (bchan, gchan, rchan) <span class="op">=</span> cv2.split(diff1)</span>
<span id="cb13-12"><a href="#cb13-12"></a>    diff2 <span class="op">=</span> bchan <span class="op">+</span> gchan <span class="op">+</span> rchan</span>
<span id="cb13-13"><a href="#cb13-13"></a>    cv2.imshow(<span class="st">"Motion"</span>, diff1)</span>
<span id="cb13-14"><a href="#cb13-14"></a>    cv2.imshow(<span class="st">"Gray sum"</span>, diff2)</span>
<span id="cb13-15"><a href="#cb13-15"></a>    x <span class="op">=</span> cv2.waitKey(<span class="dv">20</span>)</span>
<span id="cb13-16"><a href="#cb13-16"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-17"><a href="#cb13-17"></a>        c <span class="op">=</span> <span class="bu">chr</span>(x)</span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="cf">if</span> c <span class="op">==</span> <span class="st">"q"</span>:</span>
<span id="cb13-19"><a href="#cb13-19"></a>            <span class="cf">break</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>    prevFrame <span class="op">=</span> currFrame</span>
<span id="cb13-21"><a href="#cb13-21"></a></span>
<span id="cb13-22"><a href="#cb13-22"></a>cam.release()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This program computes two forms of the frame difference. The most basic one just computes the absolute value of the difference of the color images, returning a color image. The second form combines the color channels to make one grayscale difference image. <a href="#fig-motionDiff" class="quarto-xref">Figure&nbsp;6</a> shows selected results from this program.</p>
<div id="fig-motionDiff" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-motionDiff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiff280.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Color absolute difference of frames 1"><img src="Ch7-Images/absdiff280.png" class="img-fluid figure-img"></a></p>
<figcaption>Color absolute difference of frames 1</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiffsum280.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Grayscale sum of color differences 1"><img src="Ch7-Images/absdiffsum280.png" class="img-fluid figure-img"></a></p>
<figcaption>Grayscale sum of color differences 1</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiff20.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Color absolute difference of frames 3"><img src="Ch7-Images/absdiff20.png" class="img-fluid figure-img"></a></p>
<figcaption>Color absolute difference of frames 3</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiffsum20.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Grayscale sum of color differences 3"><img src="Ch7-Images/absdiffsum20.png" class="img-fluid figure-img"></a></p>
<figcaption>Grayscale sum of color differences 3</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiff120.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Color absolute difference of frames 2"><img src="Ch7-Images/absdiff120.png" class="img-fluid figure-img"></a></p>
<figcaption>Color absolute difference of frames 2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/absdiffsum120.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="Grayscale sum of color differences 2"><img src="Ch7-Images/absdiffsum120.png" class="img-fluid figure-img"></a></p>
<figcaption>Grayscale sum of color differences 2</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-motionDiff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Examples of motion detection through difference of frames, waving a hand, slight movement, and sitting still but blinking
</figcaption>
</figure>
</div>
<p><strong>Notice:</strong></p>
<ul>
<li>The color image shows the impact of the background: where motion is detected you can see a ghostly version of background objects</li>
<li>The color image shows motion less intensively than the gray version, because we add together all three channels</li>
<li>Often only the outer edges of the object in motion are visible, because central parts are still the same color</li>
</ul>
<p>We can apply thresholding, finding contours, and other techniques to these difference images to locate and identify the movement in a video.</p>
</section>
<section id="background-subtraction" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Background subtraction</h1>
<p>More elaborate methods of separating foreground from background objects exist. The best ones rely on machine learning, which we’ll talk about later on. But there are several algorithmic approaches.</p>
<p>They all have a similar form:</p>
<ul>
<li>Given a sequence of frames from a video, construct a “model” of the background of the space (based on which pixels do not change much in brightness or color).</li>
<li>For a new frame of the video, compute the difference between the background frame and the new frame.</li>
<li>Build a mask image of the pixels that are significantly different from the background: these mark the location of the foreground object</li>
<li>Apply the mask to the new frame and display the results</li>
</ul>
<p>What changes between different methods is how we build or calculate the background model.</p>
<section id="a-simple-attempt-at-background-subtraction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="a-simple-attempt-at-background-subtraction"><span class="header-section-number">3.1</span> A simple attempt at background subtraction</h2>
<p>The simplest approach is to take a picture without the foreground object in it, and just use that for the model. This has only mediocre results.</p>
<p>The code sample below uses this method to produce a masked version of the original video feed with the background set to black. It first loops on the video feed, until the user hits q. At that point, the last frame read is set up as the background picture. A more elaborate implementation would perhaps read multiple frames and average them together to form the background image.</p>
<div id="5e5bf033" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>capture <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a>capture <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="co"># Get the background as a single frame</span></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="bu">print</span>(<span class="st">"Press q when ready to take background picture"</span>)</span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb14-8"><a href="#cb14-8"></a>    gotIm, frame <span class="op">=</span> capture.read()</span>
<span id="cb14-9"><a href="#cb14-9"></a>    <span class="cf">if</span> <span class="kw">not</span> gotIm:</span>
<span id="cb14-10"><a href="#cb14-10"></a>        <span class="cf">break</span></span>
<span id="cb14-11"><a href="#cb14-11"></a></span>
<span id="cb14-12"><a href="#cb14-12"></a>    cv2.imshow(<span class="st">"Frame"</span>, frame)</span>
<span id="cb14-13"><a href="#cb14-13"></a>    x <span class="op">=</span> cv2.waitKey(<span class="dv">10</span>)</span>
<span id="cb14-14"><a href="#cb14-14"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">chr</span>(x) <span class="op">==</span> <span class="st">'q'</span>:</span>
<span id="cb14-15"><a href="#cb14-15"></a>        <span class="cf">break</span></span>
<span id="cb14-16"><a href="#cb14-16"></a></span>
<span id="cb14-17"><a href="#cb14-17"></a>backgroundPic <span class="op">=</span> frame</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After setting up the background image, the program continues in the next code sample. For each frame, it computes the difference from the background picture, and then combines the color channels together. In this case we average them together, instead of adding them as we did for motion detection above. Finally, we use a threshold to turn the difference image into a black-and-white mask, and we apply it to the frame to get just the foreground. <a href="#fig-backsub1" class="quarto-xref">Figure&nbsp;7</a> shows the background image, the foreground image, and the result of our attempt at background subtraction.</p>
<div id="2ed2fcec" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb15-2"><a href="#cb15-2"></a>    gotIm, frame <span class="op">=</span> capture.read()</span>
<span id="cb15-3"><a href="#cb15-3"></a>    <span class="cf">if</span> <span class="kw">not</span> gotIm:</span>
<span id="cb15-4"><a href="#cb15-4"></a>        <span class="cf">break</span></span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a>    fgDiff <span class="op">=</span> cv2.absdiff(frame, backgroundPic)</span>
<span id="cb15-7"><a href="#cb15-7"></a>    (b, g, r) <span class="op">=</span> cv2.split(fgDiff)</span>
<span id="cb15-8"><a href="#cb15-8"></a>    diffTot <span class="op">=</span> cv2.addWeighted(b, <span class="fl">0.33</span>, cv2.addWeighted(g, <span class="fl">0.33</span>, r, <span class="fl">0.33</span>, <span class="dv">0</span>), <span class="fl">0.66</span>, <span class="dv">0</span>)</span>
<span id="cb15-9"><a href="#cb15-9"></a>    thr, thrTot <span class="op">=</span> cv2.threshold(diffTot, <span class="dv">50</span>, <span class="dv">255</span>, cv2.THRESH_BINARY)</span>
<span id="cb15-10"><a href="#cb15-10"></a>    maskedIm <span class="op">=</span> cv2.bitwise_and(frame, frame, mask<span class="op">=</span>thrTot)</span>
<span id="cb15-11"><a href="#cb15-11"></a>    cv2.imshow(<span class="st">'Frame'</span>, frame)</span>
<span id="cb15-12"><a href="#cb15-12"></a>    cv2.imshow(<span class="st">"Masked"</span>, maskedIm)</span>
<span id="cb15-13"><a href="#cb15-13"></a>    x <span class="op">=</span> cv2.waitKey(<span class="dv">30</span>)</span>
<span id="cb15-14"><a href="#cb15-14"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">chr</span>(x) <span class="op">==</span> <span class="st">'q'</span>:</span>
<span id="cb15-15"><a href="#cb15-15"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-backsub1" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backsub1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub1BG.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Background picture"><img src="Ch7-Images/backsub1BG.png" class="img-fluid figure-img"></a></p>
<figcaption>Background picture</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub1FG.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Foreground picture"><img src="Ch7-Images/backsub1FG.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground picture</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub1Mask.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Masked result"><img src="Ch7-Images/backsub1Mask.png" class="img-fluid figure-img"></a></p>
<figcaption>Masked result</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backsub1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: The background picture, foreground (unprocessed frame), and the result of applying the simple background subtraction. Note that the pattern showing on the hand is due to the books on the bookcase matching the hand color too well.
</figcaption>
</figure>
</div>
</section>
<section id="the-mog-mixture-of-gaussians-approach-to-background-subtraction" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-mog-mixture-of-gaussians-approach-to-background-subtraction"><span class="header-section-number">3.2</span> The MOG (Mixture of Gaussians) approach to background subtraction</h2>
<p>OpenCV provides an implementation of an algorithm commonly calls “mixture of gaussians.” With this, we build a more complex model of the background of an image. By looking at a selection of recent video frames, the model starts with a range of values for each pixel in the background. They then characterize the range for each pixel as the combination, the <strong>mixture</strong>, of several gaussian functions. These allow the model to capture complex variations (like sudden brightening in one area caused by wind shifting tree branches, for example).</p>
<p>The code sample below shows how to set up and use the MOG-based background subtraction method. <a href="#fig-backsub2" class="quarto-xref">Figure&nbsp;8</a> shows the original frame and the masked result for this method.</p>
<div id="1cc4f3d2" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>backSub1 <span class="op">=</span> cv2.createBackgroundSubtractorMOG2()</span>
<span id="cb16-2"><a href="#cb16-2"></a>capture <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb16-5"><a href="#cb16-5"></a>    gotIm, frame <span class="op">=</span> capture.read()</span>
<span id="cb16-6"><a href="#cb16-6"></a>    <span class="cf">if</span> <span class="kw">not</span> gotIm:</span>
<span id="cb16-7"><a href="#cb16-7"></a>        <span class="cf">break</span></span>
<span id="cb16-8"><a href="#cb16-8"></a></span>
<span id="cb16-9"><a href="#cb16-9"></a>    fgMask1 <span class="op">=</span> backSub1.<span class="bu">apply</span>(frame)</span>
<span id="cb16-10"><a href="#cb16-10"></a>    cv2.imshow(<span class="st">"MOG2 Mask"</span>, fgMask1)</span>
<span id="cb16-11"><a href="#cb16-11"></a></span>
<span id="cb16-12"><a href="#cb16-12"></a>    maskedFrame1 <span class="op">=</span> cv2.bitwise_and(frame, frame, mask<span class="op">=</span>fgMask1)</span>
<span id="cb16-13"><a href="#cb16-13"></a>    cv2.imshow(<span class="st">'Frame'</span>, frame)</span>
<span id="cb16-14"><a href="#cb16-14"></a>    cv2.imshow(<span class="st">"Masked 1"</span>, maskedFrame1)</span>
<span id="cb16-15"><a href="#cb16-15"></a>    x <span class="op">=</span> cv2.waitKey(<span class="dv">30</span>)</span>
<span id="cb16-16"><a href="#cb16-16"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb16-17"><a href="#cb16-17"></a>        <span class="cf">if</span> <span class="bu">chr</span>(x) <span class="op">==</span> <span class="st">'q'</span>:</span>
<span id="cb16-18"><a href="#cb16-18"></a>            <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first set of photos in <a href="#fig-backsub2" class="quarto-xref">Figure&nbsp;8</a> illustrate the ideal operation of the algorithm. After letting the background sit empty for a few seconds, I then moved my arm into view. The algorithm accurately identified my arm as “foreground” and constructed a mask that correctly removed the background pixels. However, I then held my arm still in view for a few seconds. This was long enough for my arm to become incorporated as part of the background, so when I started to move my arm, the algorithm became confused overall, and failed to subtract the background properly.</p>
<div id="fig-backsub2" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backsub2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2FG0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29" title="Foreground when arm first moved into view"><img src="Ch7-Images/backsub2FG0.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground when arm first moved into view</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2Mask0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30" title="Mask created by MOG when arm first moved into view"><img src="Ch7-Images/backsub2Mask0.png" class="img-fluid figure-img"></a></p>
<figcaption>Mask created by MOG when arm first moved into view</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2Final0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Final result when arm first moved into view"><img src="Ch7-Images/backsub2Final0.png" class="img-fluid figure-img"></a></p>
<figcaption>Final result when arm first moved into view</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2FG1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-32" title="Foreground after arm stationary for a few seconds"><img src="Ch7-Images/backsub2FG1.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground after arm stationary for a few seconds</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2Mask1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-33" title="Mask created by MOG after arm stationary for a few seconds"><img src="Ch7-Images/backsub2Mask1.png" class="img-fluid figure-img"></a></p>
<figcaption>Mask created by MOG after arm stationary for a few seconds</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub2Final1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-34" title="Final result after arm stationary for a few seconds"><img src="Ch7-Images/backsub2Final1.png" class="img-fluid figure-img"></a></p>
<figcaption>Final result after arm stationary for a few seconds</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backsub2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: The original frame, the mask created from it by the MOG algorithm, and the final masked result, shown at two times: just after the arm moves into view, and after the arm is stationary for a few seconds.
</figcaption>
</figure>
</div>
</section>
<section id="the-knn-k-nearest-neighbor-approach-to-background-subtraction" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="the-knn-k-nearest-neighbor-approach-to-background-subtraction"><span class="header-section-number">3.3</span> The KNN (K-nearest neighbor) approach to background subtraction</h2>
<p>The KNN backgroung subtraction algorithm defines a sphere for each pixel. The dimensions of the sphere are the BGR values (color channels). The size of the sphere is chosen dynamically so that it encloses K of the previous frames’ colors at that pixel. You can change K to tune this algorithm. For a new frame, if the color at a pixel falls within the sphere defined for that location, then it is categorized as a background pixel, and if it falls outside of it, then it is categorized as a foreground pixel. Much like the previous algorithm, what count as background is updated dynamically to accound for lighting changes that happen gradually over time.</p>
<p>The code sample below shows how to set up and use the KNN-based background subtraction method. <a href="#fig-backsub3" class="quarto-xref">Figure&nbsp;9</a> shows the original frame and the masked result for this method.</p>
<div id="ff48abf7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>backSub2 <span class="op">=</span> cv2.createBackgroundSubtractorKNN()</span>
<span id="cb17-2"><a href="#cb17-2"></a>capture <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb17-5"><a href="#cb17-5"></a>    gotIm, frame <span class="op">=</span> capture.read()</span>
<span id="cb17-6"><a href="#cb17-6"></a>    <span class="cf">if</span> <span class="kw">not</span> gotIm:</span>
<span id="cb17-7"><a href="#cb17-7"></a>        <span class="cf">break</span></span>
<span id="cb17-8"><a href="#cb17-8"></a></span>
<span id="cb17-9"><a href="#cb17-9"></a>    fgMask2 <span class="op">=</span> backSub2.<span class="bu">apply</span>(frame)</span>
<span id="cb17-10"><a href="#cb17-10"></a>    cv2.imshow(<span class="st">"KNN Mask"</span>, fgMask2)</span>
<span id="cb17-11"><a href="#cb17-11"></a></span>
<span id="cb17-12"><a href="#cb17-12"></a>    maskedFrame2 <span class="op">=</span> cv2.bitwise_and(frame, frame, mask<span class="op">=</span>fgMask2)</span>
<span id="cb17-13"><a href="#cb17-13"></a>    cv2.imshow(<span class="st">'Frame'</span>, frame)</span>
<span id="cb17-14"><a href="#cb17-14"></a>    cv2.imshow(<span class="st">"Masked 2"</span>, maskedFrame1)</span>
<span id="cb17-15"><a href="#cb17-15"></a>    x <span class="op">=</span> cv2.waitKey(<span class="dv">30</span>)</span>
<span id="cb17-16"><a href="#cb17-16"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb17-17"><a href="#cb17-17"></a>        <span class="cf">if</span> <span class="bu">chr</span>(x) <span class="op">==</span> <span class="st">'q'</span>:</span>
<span id="cb17-18"><a href="#cb17-18"></a>            <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#fig-backsub3" class="quarto-xref">Figure&nbsp;9</a> shows the result after allowing the algorithm to see the background for a few seconds, and then moving my hand into the frame. After just a few seconds of holding still, the algorithm destabilizes. However, the KNN approach improves the longer it runs, so the third row illustrates what happens after a long time with me moving my hand in and out of view. The algorithm has re-stabilized to identify my hand better.</p>
<div id="fig-backsub3" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backsub3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3FG0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-35" title="Foreground when arm first moved into view"><img src="Ch7-Images/backsub3FG0.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground when arm first moved into view</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Mask0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-36" title="Mask created by KNN when arm first moved into view"><img src="Ch7-Images/backsub3Mask0.png" class="img-fluid figure-img"></a></p>
<figcaption>Mask created by KNN when arm first moved into view</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Final0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-37" title="Final result when arm first moved into view"><img src="Ch7-Images/backsub3Final0.png" class="img-fluid figure-img"></a></p>
<figcaption>Final result when arm first moved into view</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3FG1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-38" title="Foreground a few seconds later, when I move my arm"><img src="Ch7-Images/backsub3FG1.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground a few seconds later, when I move my arm</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Mask1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-39" title="Mask created by KNN a few seconds later"><img src="Ch7-Images/backsub3Mask1.png" class="img-fluid figure-img"></a></p>
<figcaption>Mask created by KNN a few seconds later</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Final1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-40" title="Final result a few seconds later"><img src="Ch7-Images/backsub3Final1.png" class="img-fluid figure-img"></a></p>
<figcaption>Final result a few seconds later</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3FG2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-41" title="Foreground after a longer run, with arm stationary for several seconds"><img src="Ch7-Images/backsub3FG2.png" class="img-fluid figure-img"></a></p>
<figcaption>Foreground after a longer run, with arm stationary for several seconds</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Mask2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-42" title="Mask created by KNN at that later point"><img src="Ch7-Images/backsub3Mask2.png" class="img-fluid figure-img"></a></p>
<figcaption>Mask created by KNN at that later point</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch7-Images/backsub3Final2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-43" title="Final result at that later point"><img src="Ch7-Images/backsub3Final2.png" class="img-fluid figure-img"></a></p>
<figcaption>Final result at that later point</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backsub3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: The original frame, the mask created from it by the MOG algorithm, and the final masked result, shown at three times: after just a few seconds of background when arm moves into view, after a few more seconds, when the arm moves again, and after 10-15 seconds of run time, with a stationary arm in view.
</figcaption>
</figure>
</div>
<p>A <a href="https://medium.com/@itberrios6/introduction-to-motion-detection-part-3-025271f66ef9">Medium blog by Isaac Berrios</a> does a nice job of showing how these algorithms work on a better use case: surveillance of a road. In that case, moving objects are the focus, and they will keep moving. If a car pulled over to the side of the road, however, it would disappear to become background fairly rapidly.</p>
</section>
</section>
<section id="color-tracking-with-camshift" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Color-tracking with CAMShift</h1>
<p>The previous methods looked for objects by their motion against a static background. In this approach, we will look at tracking an object by its color, in a way that is robust to some changes in lighting.</p>
<p>When we <strong>track</strong> an object as opposed to <strong>detecting</strong> an object, we keep track of its position from one frame to another rather than finding it from scratch each frame. Our previous work only <strong>detected</strong> objects, it did not take the extra step to track them. CAMshift does tracking, and it is an important part of the algorithm’s workings.</p>
<p>Color is a highly salient feature of objects in the world, and useful for tracking an object in a video signal. That said, color can vary depending on how much light, and what color of light, is lighting a scene. The technique shown here works relatively well in neutral light conditions, within a reasonable range of ambient light.</p>
<p>CAMShift stands for “Continuously Adjusting MeanShift.” So first we’ll talk about what MeanShift does, and then what CAMShift does differently. Note that both MeanShift and CAMShift are implemented by OpenCV. Meanshift is a technique that originates in statistics, as a way of analyzing a distribution of data. It just happens to apply to images under certain circumstances.</p>
<p>There is <a href="https://docs.opencv.org/4.12.0/d7/d00/tutorial_meanshift.html">an OpenCV Python tutorial about MeanShift and CAMShift</a>. It has some nice diagrams and explanations beyond what I write here.</p>
<p>Both these algorithms are built on the same underlying calculations about tracking a color or colors. Given a frame of a video, we want to compute which pixels match the colors we’re tracking, and how well they match. The section on “Histograms and Backprojection” below revisits the idea of backprojection from an earlier chapter. What we have, in the end, is a grayscale “image” where the brightness of each pixel represents how well the corresponding pixel in the frame matches our target colors. We call this the <strong>backprojection</strong>.</p>
<p>The backprojection is then used by MeanShift and CAMShift to find the central object of the target color. MeanShift moves a fixed-size tracking box around the backprojection to find the region that is most dense with the target color, and where the pixels are the strongest matches to the target color. CAMShift adjusts the size of the tracking box each time through to match the density of the backprojection.</p>
<section id="histograms-and-backprojection" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="histograms-and-backprojection"><span class="header-section-number">4.1</span> Histograms and Backprojection</h2>
<p>These algorithms use the HSV color format, and construct a histogram of hues (colors) that are the targets. The histogram is constructed by providing or selecting a <em>reference image</em>, either a whole image or a region of interest where all the pixels are the target colors. The example picture below shows a region of interest in the overall picture that includes only the orange ball. We will use this region to build the histogram.</p>
<p>To build the histogram, we: Convert the image to HSV Threshold/mask away pixels that have low saturation or low value (because very dark pixels tend to have nearly random hues assigned, so they are unreliable and noisy) Use only the resulting hue channel Have OpenCV build a histogram of the values in the hue channel</p>
<p>A histogram has a set of “bins”, and the value in each bin represents how many instances had a value in the range of that bin. Hue values will always be an integer between 0 and 179. So if we chose to have 10 bins, then all hue values between 0 and 17, inclusive, would map to the first bin; all the values between 18 and 35 would map to the second bin, and so forth.</p>
<p>For this application, we usually choose 16 or 32 bins, for no particularly good reason.</p>
<p>Thus the value in each bin of the histogram records how many pixels are within each range of hues in the target color image. Often the histogram emphasizes a single narrow range of hues, but you could also track a blue and green object with a histogram that contains both blue and green. Here is a view of the histogram that would result from the region of interest shown above.</p>
<p>And here is another example of the histogram for a different color:</p>
<p>When we want to find the target color/object in the video stream, we take a frame from the video, and extract its HSV Hue channel. Then for each pixel we compute the likelihood that its color is the target color, using the height of the bars in the histogram for that particular hue. This likelihood is stored as a grayscale “image” where strong matches are brighter than dim matches. This is called the backprojection, or backproject, of the frame. Here is a view of the backprojection for the blue image above, and also for a second image from the same set:</p>
<p>MeanShift</p>
<p>MeanShift is an algorithm used beyond computer vision; it is sometimes used in statistics or probability calculations as well. I’m just going to describe it in terms of computer vision. MeanShift takes the backproject of an image, and it starts with a fixed-size rectangular “tracking box” that is placed at some location in the image. The first time it is run, the box may be placed in any location. On subsequent runs, the box starts where it ended after the last run. MeanShift computes the “center of mass” of the backproject values in its box. That basically means the weighted average location. Imagine that the tracking box of the backproject is a flat platter, where the thickness at each point corresponds to the backproject value. The center of mass is the point where you could balance the flat platter on top of a pin.</p>
<p>If the center of mass is located at the center of the tracking box, then the algorithm ends and reports the box’s current position. If the center of mass is not at the center, then MeanShift moves its tracking box to be centered on the center of mass, and then repeats (from the step where it computed the center of mass). It repeats this process of moving the tracking box until either some maximum number of iterations occurs, or the center of mass is at the center of the box.</p>
<p>CAMShift</p>
<p>One problem with the MeanShift algorithm is that the size of the tracking box is always the same. CAMShift works just like MeanShift, except that at each step the size of the tracking box is modified. If the whole tracking box is full of the target color, then CAMShift increases the size of the box by 10% (or any percent you like). If the target color is relatively sparse in the tracking box, then it reduces the size of the box. This allows it to track objects that move closer or further from the camera.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>