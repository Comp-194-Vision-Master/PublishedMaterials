<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Susan Eileen Fox">
<meta name="dcterms.date" content="2025-10-16">

<title>Chapter 6, Image Filtering – Vision Readings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-226bd0f977fa82dfae4534cac220d79a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9011e249e8d359b0658fa71d60c1fa6f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Vision Readings</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 6, Image Filtering</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Susan Eileen Fox </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 16, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-are-image-filters" id="toc-what-are-image-filters" class="nav-link active" data-scroll-target="#what-are-image-filters"><span class="header-section-number">1</span> What are image filters?</a></li>
  <li><a href="#convolutional-filters" id="toc-convolutional-filters" class="nav-link" data-scroll-target="#convolutional-filters"><span class="header-section-number">2</span> Convolutional filters</a>
  <ul class="collapse">
  <li><a href="#a-closer-look-at-the-math-of-convolution" id="toc-a-closer-look-at-the-math-of-convolution" class="nav-link" data-scroll-target="#a-closer-look-at-the-math-of-convolution"><span class="header-section-number">2.1</span> A closer look at the math of convolution</a></li>
  <li><a href="#what-about-borders" id="toc-what-about-borders" class="nav-link" data-scroll-target="#what-about-borders"><span class="header-section-number">2.2</span> What about borders!</a></li>
  </ul></li>
  <li><a href="#blurring-filters" id="toc-blurring-filters" class="nav-link" data-scroll-target="#blurring-filters"><span class="header-section-number">3</span> Blurring filters</a></li>
  <li><a href="#morphological-filters" id="toc-morphological-filters" class="nav-link" data-scroll-target="#morphological-filters"><span class="header-section-number">4</span> Morphological filters</a></li>
  <li><a href="#edge-detection" id="toc-edge-detection" class="nav-link" data-scroll-target="#edge-detection"><span class="header-section-number">5</span> Edge Detection</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In this chapter, we will continue our examination of image transformations by exploring several different kinds of <strong>filters</strong>. Filters take in an image and produce a <em>feature map</em> that looks like an image of the same, or similar size. Each pixel in the feature map is determined by some mathematical calculations done on the neighborhood surrounding the corresponding pixel in the original image.</p>
<p>Filter tasks include morphological filters, blurring, and edge detection. These filters allow us to simplify images, drawing out interesting features while reducing the complexity of the data. Some of these filters use <strong>convolution</strong>: convolutional filters are central to modern deep learning networks that work on image data.</p>
<section id="what-are-image-filters" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What are image filters?</h1>
<p>A <strong>filter</strong> is an image transformation that converts the original image into a new image, usually called a <strong>feature map</strong>. The value at each location in the feature map is based on the pixel values in a small region surrounding that location in the original image. In other words, a filter looks at the values of pixels in a specified region around some center pixel, and it uses those values to determine the new center value in the feature map.</p>
<p>You can think of this as passing the neighborhood of values through a function that produces a single output value. <a href="#fig-filters" class="quarto-xref">Figure&nbsp;1</a> illustrates this process.</p>
<div id="fig-filters" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="Ch6-Images/filter.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: A filter takes a neighborhood around a central pixel, runs it through some transforming function, to produce a single value at the central pixel location in the feature map."><img src="Ch6-Images/filter.png" class="img-fluid figure-img" style="width:18cm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A filter takes a neighborhood around a central pixel, runs it through some transforming function, to produce a single value at the central pixel location in the feature map.
</figcaption>
</figure>
</div>
<p>A simple example of a filter is blurring. The blurred image is typically created by averaging the color values of a neighborhood around a pixel. For example, if the neighborhood was defined to be a square 3x3 region, then we would examine every 3x3 region in the image and for each:</p>
<ul>
<li>add up all the blue channel values and divide by 9 to get the new blue channel value</li>
<li>add up all the green channel values and divide by 9 to get the new blue channel value</li>
<li>add up all the red channel values and divide by 9 to get the new blue channel value</li>
<li>store the new color at the center location in the new feature map</li>
</ul>
<p>Blurring is an example of a <strong>convolutional filter.</strong> These filters have proven to be extremely important in modern deep learning models that operate on images. So we will look more closely at how they work and how they can simplify images and highlight important features in the sections below.</p>
</section>
<section id="convolutional-filters" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Convolutional filters</h1>
<p><strong>Convolutional filters</strong> are a special kind of filter, where the operation we perform on the neighborhood of pixel values is a weighted sum. We define a <em>kernel</em>: a matrix the size and shape of our neighborhood, where the values in it are weights. For every neighborhood in the image, we multiply pixel values by the corresponding weights in the kernel, and add up the results. The sum is then stored in the corresponding location in the new image/feature map.</p>
<p><a href="#fig-convProcess" class="quarto-xref">Figure&nbsp;2</a> shows the general convolutional process. The grid on the left represents a tiny image array (one channel, at least). The small 3x3 grid in the middle is the kernel of weights. Each weight is multiplied by the corresponding value in the highlighted neighborhood, and the results are added:</p>
<p><span class="math display">\[
\begin{array}{l}
\hspace{5mm} 41 \cdot 0.05 + 56 \cdot 0.15 + 71 \cdot 0.05 \\
+ 33 \cdot 0.15 + 44 \cdot 0.2 + 81 \cdot 0.15 \\
+ 27 \cdot 0.05 + 18 \cdot 0.15 + 62 \cdot 0.05
\end{array} = 47.05
\]</span></p>
<div id="fig-convProcess" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convProcess-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="Ch6-Images/convolutionalfilter.drawio.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: An example of the convolutional weighted sum at a specific location in an image"><img src="Ch6-Images/convolutionalfilter.drawio.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convProcess-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: An example of the convolutional weighted sum at a specific location in an image
</figcaption>
</figure>
</div>
<section id="a-closer-look-at-the-math-of-convolution" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-closer-look-at-the-math-of-convolution"><span class="header-section-number">2.1</span> A closer look at the math of convolution</h2>
<p>Let’s look at the math for a weighted average with a 3x3 neighborhood size.</p>
<p>In convolutional terms, the weights are in the kernel. To simplify the mathematical formula that shows how convolution works, I’m going to label the positions in the kernel in a very non-standard way (I’ll credit <a href="https://opencv.org/blog/image-filtering-using-convolution-in-opencv/">OpenCV’s blog on image filtering</a>, and other resources, for using this method to express the convolution formula):</p>
<p><span class="math display">\[
\begin{array}{cc}
K &amp; = \begin{bmatrix}
    K[-1, -1] &amp; K[-1, 0] &amp; K[-1, +1] \\
    K[0, -1] &amp; K[0, 0] &amp; K[0, +1] \\
    K[+1, -1] &amp; K[+1, 0] &amp; K[+1, +1] \\
   \end{bmatrix}
\end{array}
\]</span></p>
<p>Generally speaking, we require that the kernel’s dimensions be odd, so an <span class="math inline">\(n\)</span> by <span class="math inline">\(m\)</span> kernel can be indexed from <span class="math inline">\(-\lfloor n / 2 \rfloor\)</span> to <span class="math inline">\(+\lfloor n / 2 \rfloor\)</span> and from <span class="math inline">\(-\lfloor m / 2 \rfloor\)</span> to <span class="math inline">\(+\lfloor m / 2 \rfloor\)</span> putting a single pixel in the middle at (0, 0):</p>
<p><span class="math display">\[
\begin{array}{cc}
K &amp; = \begin{bmatrix}
    K[-\frac{n}{2}, -\frac{m}{2}] &amp; \cdots &amp; K[0, -\frac{m}{2}] &amp; \cdots &amp; K[+\frac{n}{2}, -\frac{m}{2}] \\
         \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
    K[-\frac{n}{2}, 0] &amp; \cdots &amp; K[0, 0] &amp; \cdots &amp; K[+\frac{n}{2}, 0] \\
         \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
    K[-\frac{n}{2}, +\frac{m}{2}] &amp; \cdots &amp; K[0, +\frac{m}{2}] &amp; \cdots &amp; K[+\frac{n}{2}, +\frac{m}{2}] \\
   \end{bmatrix}
\end{array}
\]</span></p>
<p>The convolution formula for an <span class="math inline">\(n\)</span> by <span class="math inline">\(m\)</span> neighborhood would look like this:</p>
<p><span class="math display">\[G[x, y] = \sum_{i=-\frac{n}{2}}^{\frac{n}{2}} \sum_{j=-\frac{m}{2}}^{+\frac{m}{2}} K[i, j] \cdot I[x - i, y - j]\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the original image, and <span class="math inline">\(G\)</span> is the resulting image.</p>
</section>
<section id="what-about-borders" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="what-about-borders"><span class="header-section-number">2.2</span> What about borders!</h2>
<p>In the figure above, I left out the border pixels from the original image. The border pixels are a special case, because they don’t have a full neighborhood around them. Because they are special, we need a special rule to manage them. Typical rules include:</p>
<ul>
<li>Making the feature map smaller than the starting image by leaving out the edge pixels (what <a href="#fig-convProcess" class="quarto-xref">Figure&nbsp;2</a> shows)</li>
<li>Computing the convolution based only on the neighbors that the edge pixels do have</li>
<li>Imagining that values continue on past the edges of the array to create full neighborhoods for edge pixels
<ul>
<li>Using interpolation/extrapolation to continue the edge colors outward as far as is needed</li>
<li>Reflecting the contents of the array outward as if it was surrounded by mirrors</li>
</ul></li>
</ul>
<p>Filtering can be used to create a number of interesting effects, but is often used to manipulate an image that is then sent to a more complex computer vision algorithm. The main purposes of filtering are:</p>
<ul>
<li>Simplifying the image by reducing color variation and/or detail</li>
<li>Highlighting small-scale patterns in the image, such as <strong>edges</strong> (places where the brightness changes across neighboring pixels), stripes, color patches, etc.</li>
</ul>
<p>In the next few sections, we will first focus on filters, both convolutional and otherwise, that simplify the image. Then examine convolutional filters that perform edge detection.</p>
</section>
</section>
<section id="blurring-filters" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Blurring filters</h1>
<p>Blurring is probably the most basic kind of filter we can do, so we will start by considering it.</p>
<p><strong>Why would we want to blur an image?</strong> As humans, we typically don’t like blurred images, unless they are blurred for some effect. However, image data to the computer is just a big box of numbers. Tiny color variations that we are barely aware of can confuse a computer vision program. When we blur an image, those tiny variations smooth out so that colors are more even, and it is easier for a computer vision system to make sense of the image data.</p>
<p>The basic blur function takes a rectangular neighborhood centered around the target pixel, and it performs the mean of the color values, channel-by-channel (all the red values are averaged, all the green values are averaged, and all the blue values are averaged). The resulting color is placed in the new image at the target pixel location. The process is repeated for every pixel in the image (with edge pixels handled specially, as described above)</p>
<p>With blurring we talk about averaging, with convolution we talk about computing a weighted sum. But really, <strong>averaging can be thought of as a weighted sum</strong>. Below I show formulas for averaging a 3x3 region, then I convert those to the weighted-sum form:</p>
<p><span class="math display">\[
\begin{array}{l}
(v_{11} + v_{12} + v_{13} + v_{21} + v_{22} + v_{23} + v_{31} + v_{32} + v_{33}) / 9 \\
\\
\hspace{1cm} = v_{11} / 9 + v_{12} / 9 + v_{13} / 9 + v_{21} / 9 + v_{22} / 9 + v_{23} / 9 + v_{31} / 9 + v_{32} / 9 + v_{33} / 9 \\
\\
\hspace{1cm} = 0.11 v_{11} + 0.11 v_{12} + 0.11 v_{13} + 0.11 v_{21} + 0.11 v_{22} + 0.11 v_{23} + 0.11 v_{31} + 0.11 v_{32} + 0.11 v_{33} \\
\end{array}
\]</span></p>
<p>The kernel for simple blurring has the same value in every cell: <span class="math inline">\(1/n\)</span>, where <span class="math inline">\(n\)</span> is the number of cells in the neighborhood.</p>
<p>OpenCV provides the <code>blur</code> function, which will perform the simple mean/averaging blur described above. We specify the image to be blurred, and a tuple describing the neighborhood size, it returns a blurred image.</p>
<p>The code block below are some example of how to call the <code>blur</code> function. <a href="#fig-blurResults" class="quarto-xref">Figure&nbsp;5</a> shows the original and resulting images.</p>
<div id="57c315ab" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>image <span class="op">=</span> cv2.imread(<span class="st">"SampleImages/butterfly.jpg"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a>blur1 <span class="op">=</span> cv2.blur(image, (<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb1-4"><a href="#cb1-4"></a>blur2 <span class="op">=</span> cv2.blur(image, (<span class="dv">21</span>, <span class="dv">21</span>))</span>
<span id="cb1-5"><a href="#cb1-5"></a>blur3 <span class="op">=</span> cv2.blur(image, (<span class="dv">3</span>, <span class="dv">41</span>))</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>cv2.imshow(<span class="st">"Original"</span>, image)</span>
<span id="cb1-8"><a href="#cb1-8"></a>cv2.imshow(<span class="st">"Blur1 feature map"</span>, blur1)</span>
<span id="cb1-9"><a href="#cb1-9"></a>cv2.imshow(<span class="st">"Blur2 feature map"</span>, blur2)</span>
<span id="cb1-10"><a href="#cb1-10"></a>cv2.imshow(<span class="st">"Blur3 feature map"</span>, blur3)</span>
<span id="cb1-11"><a href="#cb1-11"></a>cv2.waitKey()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-blurResults" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blurResults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch5-Images/butterfly.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Original image"><img src="Ch5-Images/butterfly.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Original image</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/blurRes7x7.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Blur image (simple mean) with neighborhood of (7, 7)"><img src="Ch6-Images/blurRes7x7.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Blur image (simple mean) with neighborhood of (7, 7)</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/blurRes21x21.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Blur image (simple mean) with neighborhood of (21, 21)"><img src="Ch6-Images/blurRes21x21.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Blur image (simple mean) with neighborhood of (21, 21)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/blurRes3x41.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Blur image (simple mean) with neighborhood of (3, 41) (tall but narrow neighborhood)"><img src="Ch6-Images/blurRes3x41.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Blur image (simple mean) with neighborhood of (3, 41) (tall but narrow neighborhood)</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blurResults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Three different blurs of an image, demonstrating different neighborhood sizes: a small-ish one, a fairly large one, and a non-square one that is very narrow but very tall, leading to a vertical smearing effect
</figcaption>
</figure>
</div>
<p>Besides taking a straight average of the color channel values, we can get a blur effect by taking a <strong>weighted</strong> average of the values in the neighborhood. To do a weighted average with convolution without altering the brightness of the image, we add a constraint that says that the values in the kernel must add up to 1.0:</p>
<p><span class="math display">\[\sum_{i=-\frac{n}{2}}^{\frac{n}{2}} \sum_{j=-\frac{m}{2}}^{+\frac{m}{2}} K[i, j] = 1.0\]</span></p>
<p>OpenCV provides a weighted blur function called <code>GaussianBlur</code>. This function does a weighted average where the weights are chosen according to a two-dimensional Gaussian function with its peak centered on the center pixel of the neighborhood. Gaussian functions, or curves, are often called <em>normal curves</em> or <em>bell curves</em>. In Chapter 4 we briefly talked about 2d Gaussian curves in the context of the <code>adaptiveThreshold</code> function. <a href="#fig-gauss" class="quarto-xref">Figure&nbsp;4</a> illustrates the shape.</p>
<div id="fig-gauss" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gauss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="Ch4-Images/2dGaussPlot.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;4: Two-dimensional Gaussian curve"><img src="Ch4-Images/2dGaussPlot.png" class="img-fluid figure-img" style="width:14cm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gauss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Two-dimensional Gaussian curve
</figcaption>
</figure>
</div>
<p>We scale the Gaussian curve to fit our neighborhood size, and so that its values at discrete points add up to one, and those values become our kernel values. <strong>Note:</strong> <a href="#fig-convProcess" class="quarto-xref">Figure&nbsp;2</a> uses a kernel that approximates the gaussian blur kernel for a 3x3 neighborhood. The shape of the Gaussian curve is defined by its standard deviation in the x and y directions.</p>
<p>OpenCV provides the <code>GaussianBlur</code> function, which will perform the Gaussian weighted average blur described above. We specify the image to be blurred, a tuple describing the neighborhood size (dimensions must be odd!) and a value for the standard deviation in the x direction. If we pass in zero, then the function determines the best standard deviation values based on the width and height of the neighborhood.</p>
<p>The code block below are some example of how to call the <code>Gaussian</code> function. <strong>?@fig-gaussBlurResults</strong> shows the original and resulting images.</p>
<div id="1551d17e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>image <span class="op">=</span> cv2.imread(<span class="st">"SampleImages/butterfly.jpg"</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>gblur1 <span class="op">=</span> cv2.GaussianBlur(image, (<span class="dv">7</span>, <span class="dv">7</span>), <span class="dv">0</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>gblur2 <span class="op">=</span> cv2.GaussianBlur(image, (<span class="dv">21</span>, <span class="dv">21</span>), <span class="dv">0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a>gblur3 <span class="op">=</span> cv2.GaussianBlur(image, (<span class="dv">3</span>, <span class="dv">41</span>), <span class="dv">0</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>cv2.imshow(<span class="st">"Original"</span>, image)</span>
<span id="cb2-8"><a href="#cb2-8"></a>cv2.imshow(<span class="st">"GBlur1 feature map"</span>, gblur1)</span>
<span id="cb2-9"><a href="#cb2-9"></a>cv2.imshow(<span class="st">"GBlur2 feature map"</span>, gblur2)</span>
<span id="cb2-10"><a href="#cb2-10"></a>cv2.imshow(<span class="st">"GBlur3 feature map"</span>, gblur3)</span>
<span id="cb2-11"><a href="#cb2-11"></a>cv2.waitKey()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-blurResults" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blurResults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/butterfly.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Original image"><img src="Ch6-Images/butterfly.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Original image</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/gaussBlurRes7x7.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Gaussian Blur image with neighborhood of (7, 7)"><img src="Ch6-Images/gaussBlurRes7x7.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Gaussian Blur image with neighborhood of (7, 7)</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/gaussBlurRes21x21.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Gaussian Blur image with neighborhood of (21, 21)"><img src="Ch6-Images/gaussBlurRes21x21.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Gaussian Blur image with neighborhood of (21, 21)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Ch6-Images/gaussBlurRes3x41.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Gaussian Blur image with neighborhood of (3, 41) (tall but narrow neighborhood)"><img src="Ch6-Images/gaussBlurRes3x41.jpg" class="img-fluid figure-img"></a></p>
<figcaption>Gaussian Blur image with neighborhood of (3, 41) (tall but narrow neighborhood)</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blurResults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Three different Gaussian blurs of an image, demonstrating different neighborhood sizes: a small-ish one, a fairly large one, and a non-square one that is very narrow but very tall, leading to a vertical smearing effect
</figcaption>
</figure>
</div>
<p><strong>Why use Gaussian Blur?</strong> Compare the results of the Gaussian blur with the simple blur results from earlier in this section. Notice that the Gaussian blur retains more of the detailed features of the image. In practice, Gaussian blurs often do better at simplifying images the way we want, reducing color variations, noise, and tiny zigzags, while preserving more of the features we want to keep, such as edges. There are more elaborate blurring functions as well, including <code>medianFilter</code> and <code>bilateralFilter</code>, but we won’t dig too deeply into those here.</p>
<p>In the <code>SampleCode</code> folder that was provided to you, there is a demo program called <code>blurring.py</code> that you can use to explore these two blurring functions, and the effects of different neighborhood sizes. Run the program, and then use the following keyboard commands to control the effect:</p>
<ul>
<li>1: switches to the simple blur</li>
<li>2: switches to the Gaussian blur</li>
<li>w: increases the height of the neighborhood by 2</li>
<li>s: decreases the height of the neighborhood by 2</li>
<li>a: decreases the width of the neighborhood by 2</li>
<li>d: increases the width of the neighborhood by 2</li>
<li>q: quits the program</li>
</ul>
</section>
<section id="morphological-filters" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Morphological filters</h1>
<p>There are a set of “morphological” filters to experiment with. The program simpleMorph.py demonstrates how to select from the different morphological filters (described below), and how to build the neighborhood object, and call the morphologyEx function. Try the different filters with different neighborhood sizes and shapes, and note the effect.</p>
<p>Dilation and Erosion Much like blurring, dilation and erosion determine the value for a pixel based on a neighborhood of pixels from the original image. Unlike the default in blurring, we can select the shape of the neighborhood, as well as its size, to be either rectangular, elliptical, or cross shaped. With dilation, the value of each channel of a pixel is the maximum value of that channel in any pixel in its neighborhood. Erosion is the opposite: the value of each channel of a pixel is the minimum value of that channel in any pixel in its neighborhood.</p>
<p>These can be used to emphasize and thicken edges or regions of color in a particular part of an image.</p>
<p>Opening and Closing Sometimes we want to preserve both dark and light features of an image. Opening and closing combine dilation and erosion. Opening an image means first performing an erosion of the image, then a dilation, using the same size and shape of neighborhood. Closing first dilates the image, then erodes it.</p>
<p>Both these operations are good at removing noise and small details from images.</p>
<p>White and Black Top-Hat The Top-Hat filters do the opposite of opening and closing. Instead of removing the fine details, these filters keep just the fine details. The white top-hat takes the difference between the original image and the opening of the image. The black top-hat takes the difference between the closing and the original image.</p>
<p>These filters may be used for feature extraction tasks, or image enhancement.</p>
<p>Morphological Gradient The morphological gradient takes the difference between the dilation and erosion of an image. This emphasizes the places where there is a change in color, and can be useful to enhance edges.</p>
<p>Milestone 3: Blurring and morphing video</p>
<p>Start with a program that just displays the frames from the webcam. Once again, you may pick just one of these options to complete.</p>
<p>Blurring the video gradually: Use the Gaussian blur function to blur the image to be displayed. Define a variable blurDir to be 2. This will be how much we will change the neighborhood size each time through the loop Set up a variable kSize to hold the current neighborhood size, initially set to 1 or 3 Use Gaussian blur to blur the image with the current size kSize (and display its results) Update the current size by adding blurDir to it. If the resulting kSize is greater than some maximum size (try 50 or 100) then change blurDir to -2 If the resulting kSize is less than 3, then set blurDir to 2</p>
<p>Morphing the video, instead: Look at the simpleMorph.py sample program for how to use the morphological filters. Choose at least 1 of the morphological filters described above and demonstrated in morph.py. Modify the blurring program so that it operates with a changing neighborhood size but applies the chosen morphological filter instead of blurring. Try changing the neighborhood shape to see what effect it has.</p>
</section>
<section id="edge-detection" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Edge Detection</h1>
<p>Edge Detection</p>
<p>Finding edges and lines in images is one of the classic computer vision tasks. Edges are defined to be locations in an image where the color or brightness changes dramatically. There are different tools for determining these edges; we’ll look at the Sobel gradient operation, and the Canny edge detection algorithm.</p>
<p>For several of these tools, we treat the brightness across the pixels of the image as a surface, a two-dimensional function and apply some calculus tools to it. Fortunately, you don’t have to know multivariate calculus to understand what the methods are doing.</p>
<p>Sobel Gradient-Finding</p>
<p>Sobel first uses Gaussian blurring on an image to smooth the surfaces in the image, and then it looks for peaks and valleys in the result. In mathematical terms, it computes the first derivative of the function represented by the smoothed image at each pixel location. Thus the result of Sobel is a floating-point number, which we must convert into a grayscale value for displaying it.</p>
<p>Sobel is an example of a convolutional filter, much like blurring. Its kernel produces floating-point values with a large magnitude (large positive or very small negative) when there is a change in brightness. Sobel must be computed in horizontal and vertical directions differently. Below is an example of a 3x3 kernels for Sobel:</p>
<p>Finds vertical edges 1 0 -1 2 0 -2 1 0 -1</p>
<p>Finds horizontal edges 1 2 1 0 0 0 -1 -2 -1</p>
<p>It works better to compute the Sobel values in two directions separately, and then to combine the two. Note that, for precision, the type of data in the gradients is 32-bit floating-point numbers, which are then scaled into unsigned 8-bit numbers for display.</p>
<div id="c7805220" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> cv2</span>
<span id="cb3-2"><a href="#cb3-2"></a>img <span class="op">=</span> cv2.imread(<span class="st">"SampleImages/chicago.jpg"</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>gray <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co"># Compute gradient in horizontal direction (detects vertical edges)</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>sobelValsHorz <span class="op">=</span> cv2.Sobel(gray, cv2.CV_32F, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a>horzImg <span class="op">=</span> cv2.convertScaleAbs(sobelValsHorz)</span>
<span id="cb3-8"><a href="#cb3-8"></a>cv2.imshow(<span class="st">"horizontal gradient"</span>, horzImg)</span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co"># Compute gradient in vertical direction (Detects horizontal edges)</span></span>
<span id="cb3-11"><a href="#cb3-11"></a>sobelValsVerts <span class="op">=</span> cv2.Sobel(gray, cv2.CV_32F, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb3-12"><a href="#cb3-12"></a>vertImg <span class="op">=</span> cv2.convertScaleAbs(sobelValsVerts)</span>
<span id="cb3-13"><a href="#cb3-13"></a>cv2.imshow(<span class="st">"vertical gradient"</span>, vertImg)</span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co"># Combine the two gradients</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>sobelComb <span class="op">=</span> cv2.addWeighted(sobelValsHorz, <span class="fl">0.5</span>, sobelValsVerts, <span class="fl">0.5</span>, <span class="dv">0</span>)</span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co"># Convert back to uint8</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>sobelImg <span class="op">=</span> cv2.convertScaleAbs(sobelComb)</span>
<span id="cb3-19"><a href="#cb3-19"></a>cv2.imshow(<span class="st">"Sobel"</span>, sobelImg)</span>
<span id="cb3-20"><a href="#cb3-20"></a>cv2.waitKey(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Try the example above. Then try modifying things. What happens if you put -1 in for the cv2.CV_32F, which causes the resulting gradients to use the same type as the images themselves, unsigned 8-bit numbers? What if you change the weighting in the addWeighted function? What if you don’t convert back to unsigned 8-bit values? What if you use the color image? Or the hue channel from an HSV image?</p>
<p>Canny Edge Detection</p>
<p>The Canny algorithm goes beyond what Sobel does. It performs a Sobel transformation in both horizontal and vertical directions as its first step. It then removes points that have a change in brightness, but that are not “local maxima” (whose neighbors in the direction of the change are not all smaller than it). Finally, it has two threshold values (supplied as inputs). Any edge value below the min threshold is discarded. Any edge value above the max threshold is kept. In between the two thresholds, edges are kept only if they are connected to some edge above the threshold.</p>
<p>The Canny algorithm takes a grayscale picture as input, and the two thresholds.</p>
<div id="64a4ed96" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> cv2</span>
<span id="cb4-2"><a href="#cb4-2"></a>img <span class="op">=</span> cv2.imread(<span class="st">"SampleImages/chicago.jpg"</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>gray <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>cannyImg <span class="op">=</span> cv2.Canny(gray, <span class="dv">100</span>, <span class="dv">200</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>cv2.imshow(<span class="st">"Canny"</span>, cannyImg)</span>
<span id="cb4-7"><a href="#cb4-7"></a>cv2.waitKey(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One of the difficulties with Canny edge detection is determining the proper threshold values. Write a program that loops through possible threshold values in increments of 5 or 10 (the threshold value must be between 0 and 255, and one value must be larger than the other), and displays the edges that are found. Decide what produced the “best” value. Be sure to try a variety of pictures in the SampleImages folder, to see how results vary across different kinds of images.</p>
<p>Create your own program to show the video image from your computer’s camera, displaying the result of Canny edge detection.</p>
<p>There are line detection algorithms, Hough (pronounced “huff”) lines and circles, which you may experiment with. We are going to skip them and focus on corners instead of edges.</p>
<p>Milestone 5: Finding edges</p>
<p>Experiment with edge detection, both Sobel and Canny, applied to the images and videos of the balls. Be sure to convert the images to grayscale before passing them to Sobel or Canny. Try adjusting the parameters for Canny: can you get it to produce useful edges? What if you use preprocessing (thresholding, masking, morphological filters) prior to calling Canny? How well would it work as a tool to find the interesting objects in each scene?</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>